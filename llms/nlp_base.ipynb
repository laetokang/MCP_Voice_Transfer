{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8270fec-1b93-4f96-bf86-65942c31b532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.6.0 fsspec-2025.3.2 huggingface-hub-0.30.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f471cd7e-e674-4f4d-b060-60714bc3a1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 버전: 2.1.0+cu118\n",
      "GPU 사용 가능 여부: True\n",
      "GPU 이름: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"GPU 사용 가능 여부:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU 이름:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ca578b-60c1-49ed-878a-569c91012549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ee4ac4-0796-41ca-bf9b-c25696f3b9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230255136c5f4e2b92703f79a5758906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): Phi3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            torch_dtype=torch.float16)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae9b633-892a-4fba-a936-04912cbd4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_a(input_text):\n",
    "    return f\"\"\"\n",
    "다음 문장을 분석하여 intent, amount, recipient, response를 예시 형식으로 출력해 주세요.\n",
    "\n",
    "intent는 다음 중 하나입니다: transfer, confirm, cancel, other  \n",
    "amount는 숫자만 (없으면 None)  \n",
    "recipient는 사람 이름 등 (없으면 None)  \n",
    "response는 고객님에게 안내할 자연스러운 한국어 문장\n",
    "\n",
    "예시:  \n",
    "text: \"아빠한테 오만원 보내줘\"  \n",
    "출력:{{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"엄마\",\"response\": \"엄마님께 30,000원을 송금해드릴까요?\"}}  \n",
    "\n",
    "text: \"{input_text}\"\n",
    "\"\"\"\n",
    "\n",
    "def prompt_b(input_text):\n",
    "    return f\"\"\"\n",
    "다음 문장을 분석하여 intent, amount, recipient, response를 JSON형식으로 출력해 주세요.\n",
    "\n",
    "intent는 다음 중 하나입니다:  \n",
    "  transfer: 사용자가 누군가에게 금전을 보내고자 하는 의도,  \n",
    "  confirm: 이전 발화를 확인하거나 반복하는 표현,  \n",
    "  cancel: 이전 동작을 취소하거나 거절하려는 의도,  \n",
    "  inquiry: 송금과 관련된 정보를 묻거나 확인하는 의도,  \n",
    "  other: 시스템과 무관한 일상 대화 or 분류 불가능한 문장,  \n",
    "  system_response: 시스템이 사용자에게 재질문하거나 안내하는 응답 발화  \n",
    "amount는 숫자만 (없으면 None)  \n",
    "recipient는 사람 이름 등 (없으면 None)  \n",
    "response는 고객님에게 안내할 자연스러운 한국어 문장, None값이 있다면 꼬리질문을 해서 none값을 채우세요\n",
    "\n",
    "예시:  \n",
    "text: \"아빠한테 오만원 보내줘\"  \n",
    "{{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"엄마\",\"response\": \"엄마님께 30,000원을 송금해드릴까요?\"}}  \n",
    "\n",
    "text: \"{input_text}\"\n",
    "\"\"\"\n",
    "\n",
    "def prompt_c(input_txt):\n",
    "    return  f\"\"\"\n",
    "    다음 문장을 분석하여 intent, amount, recipient, response를 예시 형식을 따라 추출해 주세요.\n",
    "\n",
    "    intent는 다음 중 하나입니다: transfer, confirm, cancel, 송금 관련 정보를 물을 시 inquiry,송금 외 질문을 할 시 other\n",
    "    amount는 숫자만 (없으면 None)\n",
    "    recipient는 사람 이름 등 (없으면 None)\n",
    "    response는 고객님에게 안내할 자연스러운 한국어 문장, None값이 있다면 질문을 해서 값을 채울 것\n",
    "\n",
    "예시:\n",
    "text: \"엄마한테 삼만원 보내줘\"\n",
    "\n",
    "  {{\"intent\": \"transfer\",\"amount\": 30000,\"recipient\": \"엄마\",\"response\": \"엄마님께 30,000원을 송금해드릴까요?\"}}\n",
    "\n",
    "\n",
    "{input_text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d16329ab-940a-42fc-9149-3da3280739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_inference.py\n",
    "def run_inference(prompt: str, tokenizer, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    start = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens=80,do_sample=False)\n",
    "    end = time.time()\n",
    "\n",
    "    # 출력 해석\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    output_text = generated.replace(prompt, \"\").strip()\n",
    "\n",
    "    print(\"\\n📦 LLM 원문 출력:\")\n",
    "    print(output_text)\n",
    "\n",
    "\n",
    "    try:\n",
    "        json_start = output_text.find(\"{\")\n",
    "        json_end = output_text.rfind(\"}\") + 1\n",
    "        json_block = output_text[json_start:json_end]\n",
    "        parsed = json.loads(json_block)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"JSON 파싱 실패:\", e)\n",
    "        print(\"📦 원문 출력:\", repr(output_text))\n",
    "        parsed = {}\n",
    "    \n",
    "    print(f\"\\n⏱️ 처리 시간: {round(end - start, 2)}초\")\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64c14e4-630e-49c7-b557-c2112b89902c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7a5871e1a441b6acd6061563c06339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 15.63 GiB of which 189.69 MiB is free. Process 3977112 has 15.44 GiB memory in use. Of the allocated memory 15.23 GiB is allocated by PyTorch, and 13.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. 입력 문장\u001b[39;00m\n\u001b[1;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m아빠한테 오천 원 보내줘\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3698\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3697\u001b[0m         )\n\u001b[0;32m-> 3698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacty of 15.63 GiB of which 189.69 MiB is free. Process 3977112 has 15.44 GiB memory in use. Of the allocated memory 15.23 GiB is allocated by PyTorch, and 13.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from prompt_templates import prompt_a\n",
    "# from run_inference import run_inference\n",
    "\n",
    "# # 1. 모델 로딩\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "# 2. 입력 문장\n",
    "text = \"아빠한테 오천 원 보내줘\"\n",
    "prompt = prompt_a(text)\n",
    "\n",
    "# 3. 단독 인퍼런스 실행\n",
    "result = run_inference(prompt, tokenizer, model)\n",
    "\n",
    "print(\"\\n🎯 최종 출력 결과:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11cf255-ba3a-418f-a737-6373cf165a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_prompt.py\n",
    "\n",
    "def evaluate_prompt(prompt_func, input_data,tokenizer,model):\n",
    "    if isinstance(input_data, dict):\n",
    "        input_data = [input_data]\n",
    "\n",
    "    results = []\n",
    "    for s in input_data:\n",
    "        prompt = prompt_func(s[\"text\"])\n",
    "        output = run_inference(prompt,tokenizer,model)\n",
    "        results.append(output)\n",
    "\n",
    "        # 🔥  결과 출력!\n",
    "        print(f\"\\n[입력문장] {s['text']}\")\n",
    "        print(\"정답:\", s[\"intent\"], s.get(\"slots\", {}))\n",
    "        print(\"출력:\")\n",
    "        pprint(output)\n",
    "\n",
    "    def clean(s): return str(s).strip().lower() if s else \"\"\n",
    "    def to_int(s): \n",
    "        try: return int(str(s).replace(\",\", \"\").strip())\n",
    "        except: return None\n",
    "\n",
    "    total = len(input_data)\n",
    "\n",
    "    metrics = {\n",
    "        \"intent_acc\": sum(clean(r.get(\"intent\")) == clean(s[\"intent\"]) for r, s in zip(results, input_data)) / total,\n",
    "        \"amount_acc\": sum(to_int(r.get(\"amount\")) == s[\"slots\"].get(\"amount\") for r, s in zip(results, input_data)) / total,\n",
    "        \"recipient_acc\": sum(clean(r.get(\"recipient\")) == clean(s[\"slots\"].get(\"recipient\")) for r, s in zip(results, input_data)) / total,\n",
    "        \"json_success_rate\": sum(bool(r) for r in results) / total\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c426ed9-98cd-4699-94e4-f34596bcb9d0",
   "metadata": {},
   "source": [
    "### run inference 작성 완 -> testing 하면 됨\n",
    "아래는 검토할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c293b-0d8d-419e-8aa0-32a9c9aef35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ evaluate_prompt.py\n",
    "\n",
    "def evaluate_prompt(prompt_func, input_data, tokenizer, model):\n",
    "    if isinstance(input_data, dict):\n",
    "        input_data = [input_data]\n",
    "\n",
    "    results = []\n",
    "    total_time = 0\n",
    "    each_times = []\n",
    "\n",
    "    for s in input_data:\n",
    "        prompt = prompt_func(s[\"text\"])\n",
    "        output, elapsed_time = run_inference(prompt, tokenizer, model)\n",
    "        results.append(output)\n",
    "        each_times.append(elapsed_time)\n",
    "        total_time += elapsed_time\n",
    "\n",
    "        print(f\"\\n[입력문장] {s['text']}\")\n",
    "        print(\"🎯 정답:\", s[\"intent\"], s.get(\"slots\", {}))\n",
    "        print(\"🧠 출력:\", output)\n",
    "        print(f\"⏱️ 소요 시간: {elapsed_time}초\")\n",
    "\n",
    "    def clean(s): return str(s).strip().lower() if s else \"\"\n",
    "    def to_int(s):\n",
    "        try: return int(str(s).replace(\",\", \"\").strip())\n",
    "        except: return None\n",
    "\n",
    "    total = len(input_data)\n",
    "\n",
    "    metrics = {\n",
    "        \"intent_acc\": sum(clean(r.get(\"intent\")) == clean(s[\"intent\"]) for r, s in zip(results, input_data)) / total,\n",
    "        \"amount_acc\": sum(to_int(r.get(\"amount\")) == s[\"slots\"].get(\"amount\") for r, s in zip(results, input_data)) / total,\n",
    "        \"recipient_acc\": sum(clean(r.get(\"recipient\")) == clean(s[\"slots\"].get(\"recipient\")) for r, s in zip(results, input_data)) / total,\n",
    "        \"json_success_rate\": sum(bool(r) for r in results) / total,\n",
    "        \"avg_response_time\": round(total_time / total, 2)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"results\": results,\n",
    "        \"times\": each_times\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ main.py (사용 예시)\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from prompt_templates import prompt_a, prompt_b\n",
    "# from evaluate_prompt import evaluate_prompt\n",
    "# from run_inference import run_inference\n",
    "\n",
    "# model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# model.to(\"cuda\")\n",
    "\n",
    "# samples = [\n",
    "#     {\n",
    "#         \"text\": \"엄마한테 삼만 원 보내줘\",\n",
    "#         \"intent\": \"transfer\",\n",
    "#         \"slots\": {\"recipient\": \"엄마\", \"amount\": 30000}\n",
    "#     },\n",
    "#     {\n",
    "#         \"text\": \"그냥 넘어가\",\n",
    "#         \"intent\": \"cancel\",\n",
    "#         \"slots\": {}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# result_a = evaluate_prompt(prompt_a, samples, tokenizer, model)\n",
    "# result_b = evaluate_prompt(prompt_b, samples, tokenizer, model)\n",
    "\n",
    "# print(\"\\n📊 프롬프트 성능 비교:\")\n",
    "# for k in result_a[\"metrics\"]:\n",
    "#     print(f\"{k:<18}: A={result_a['metrics'][k]*100:.1f}%   |   B={result_b['metrics'][k]*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddbd999-12dc-49b8-8f53-d74727520cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=[\n",
    "  {\n",
    "    \"text\": \"철수한테 오천 원 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"철수\", \"amount\": 5000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"엄마한테 3만원만\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"엄마\", \"amount\": 30000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"만원만 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": 10000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"엄마한테 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"엄마\", \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"나 송금하고 싶어\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"가스비 내야 하는데 어떻게 해야 해?\",\n",
    "    \"intent\": \"질문\",\n",
    "    \"slots\": { \"topic\": \"가스비\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"내 계좌에 얼마 남았어?\",\n",
    "    \"intent\": \"질문\",\n",
    "    \"slots\": { \"topic\": \"잔액조회\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"그 사람한테 또 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"이전대화\", \"amount\": \"이전대화\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"아까랑 똑같이 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"이전대화\", \"amount\": \"이전대화\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"송금할래\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": None, \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"이체 수수료 얼마야?\",\n",
    "    \"intent\": \"질문\",\n",
    "    \"slots\": { \"topic\": \"이체 수수료\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"오늘 아빠랑 점심 먹었어\",\n",
    "    \"intent\": \"기타\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"아빠한테 이만원 보내줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"아빠\", \"amount\": 20000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"여보한테 십만 원 이체해줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"여보\", \"amount\": 100000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"보내지 마\",\n",
    "    \"intent\": \"취소\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"그냥 넘어가줘\",\n",
    "    \"intent\": \"취소\",\n",
    "    \"slots\": {}\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"엄마 송금해줘\",\n",
    "    \"intent\": \"송금\",\n",
    "    \"slots\": { \"recipient\": \"엄마\", \"amount\": None }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"아, 삼만원 보내는 거였지\",\n",
    "    \"intent\": \"확인\",\n",
    "    \"slots\": { \"amount\": 30000 }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"예금해줘\",\n",
    "    \"intent\": \"기타\",\n",
    "    \"slots\": { \"action\": \"예금\" }\n",
    "  },\n",
    "  {\n",
    "    \"text\": \"누구한테 보낼까요?\",\n",
    "    \"intent\": \"시스템응답\",\n",
    "    \"slots\": {}\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42be50d-3337-4e2a-9f3f-1f7254435f8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_a = evaluate_prompt(prompt_a, samples[0])\n",
    "result_b = evaluate_prompt(prompt_b, samples[0])\n",
    "\n",
    "print(\"\\n📊 프롬프트 성능 비교:\")\n",
    "for k in result_a[\"metrics\"]:\n",
    "    print(f\"{k:<18}: A={result_a['metrics'][k]*100:.1f}%   |   B={result_b['metrics'][k]*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c678dcce-2bac-48e6-9b0c-c87649b7f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 문장: 철수한테 오천 원 보내줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 100000,\n",
      "  \"recipient\": \"철수\",\n",
      "  \"response\": \"철수님께 천원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 4.07초\n",
      "\n",
      "[2] 문장: 엄마한테 3만원만\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: 'text: \"저는 송금 예정이에요\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"송금 예정이에요네요, 언제 송금'\n",
      "\n",
      "⏱️ 처리 시간: 3.91초\n",
      "\n",
      "[3] 문장: 만원만 보내줘\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 52 (char 51)\n",
      "📦 원문 출력: '{\"intent\": \"transfer\",\"amount\": 10000,\"recipient\": None,\"response\": \"10,000원을 송금하시겠습니까?\"}\\n\\n\\n송금하고 싶어요\\n\\n  {\"intent\": \"'\n",
      "\n",
      "⏱️ 처리 시간: 3.94초\n",
      "\n",
      "[4] 문장: 엄마한테 보내줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"엄마\",\n",
      "  \"response\": \"엄마님께 10,000원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 4.09초\n",
      "\n",
      "[5] 문장: 나 송금하고 싶어\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: ''\n",
      "\n",
      "⏱️ 처리 시간: 4.11초\n",
      "\n",
      "[6] 문장: 가스비 내야 하는데 어떻게 해야 해?\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 32 (char 31)\n",
      "📦 원문 출력: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"가스비를 준비하려면 저희에게 더 자세한 정보를 알려주세요.\"}��������'\n",
      "\n",
      "⏱️ 처리 시간: 4.15초\n",
      "\n",
      "[7] 문장: 내 계좌에 얼마 남았어?\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 32 (char 31)\n",
      "📦 원문 출력: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"내 계좌에는 얼마나 남았나요?\"}\\n\\n\\n송금하고 싶어요.\\n\\n  {\"intent\": \"transfer\",\"amount\": None,\"'\n",
      "\n",
      "⏱️ 처리 시간: 4.13초\n",
      "\n",
      "[8] 문장: 그 사람한테 또 보내줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"그런 사람\",\n",
      "  \"response\": \"그런 사람님께 10,000원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 3.97초\n",
      "\n",
      "[9] 문장: 아까랑 똑같이 보내줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"아까랑\",\n",
      "  \"response\": \"아까랑 10,000원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 4.02초\n",
      "\n",
      "[10] 문장: 송금할래\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: '송금 관련 정보를 물을 시 inquiry\\n\\n송금 관련 정보를 물을 시 inquiry\\n\\n송금 관련 정보를 물을'\n",
      "\n",
      "⏱️ 처리 시간: 4.01초\n",
      "\n",
      "[11] 문장: 이체 수수료 얼마야?\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 32 (char 31)\n",
      "📦 원문 출력: '{\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"이 수수료는 얼마인가요?\"}\\n\\n\\n송금 예정일은 몇 일이야?\\n\\n  {\"intent\": \"inquiry\",\"amount'\n",
      "\n",
      "⏱️ 처리 시간: 4.11초\n",
      "\n",
      "[12] 문장: 오늘 아빠랑 점심 먹었어\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"other\",\n",
      "  \"response\": \"오늘 아빠랑 점심 먹었어요, 잘 지냈어요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 4.01초\n",
      "\n",
      "[13] 문장: 아빠한테 이만원 보내줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 10000,\n",
      "  \"recipient\": \"아빠\",\n",
      "  \"response\": \"아빠님께 1만원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 3.98초\n",
      "\n",
      "[14] 문장: 여보한테 십만 원 이체해줘\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 100000,\n",
      "  \"recipient\": \"여보\",\n",
      "  \"response\": \"여보님께 100,000원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 3.87초\n",
      "\n",
      "[15] 문장: 보내지 마\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: '예시:\\ntext: \"저희 송금 서비스 알려주세요\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"저희 송금 서비스'\n",
      "\n",
      "⏱️ 처리 시간: 3.89초\n",
      "\n",
      "[16] 문장: 그냥 넘어가줘\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: 'text: \"저희 회사에서 송금하려고 합니다\"\\n\\n  {\"intent\": \"transfer\",\"amount\": None,\"recipient\": None,\"response\": \"저희 회사에서 송금하려면 �'\n",
      "\n",
      "⏱️ 처리 시간: 3.92초\n",
      "\n",
      "[17] 문장: 엄마 송금해줘\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 32 (char 31)\n",
      "📦 원문 출력: 'text: \"저는 송금하고 싶어요\"\\n\\n  {\"intent\": \"inquiry\",\"amount\": None,\"recipient\": None,\"response\": \"송금하고 싶은 사람이 뭔가요?\"}����'\n",
      "\n",
      "⏱️ 처리 시간: 3.86초\n",
      "\n",
      "[18] 문장: 아, 삼만원 보내는 거였지\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"transfer\",\n",
      "  \"amount\": 3000000,\n",
      "  \"recipient\": \"엄마\",\n",
      "  \"response\": \"엄마님께 3,000,000원을 송금해드릴까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 3.85초\n",
      "\n",
      "[19] 문장: 예금해줘\n",
      "❌ JSON 파싱 실패: Expecting value: line 1 column 1 (char 0)\n",
      "📦 원문 출력: '엄마한테 송금해\\n\\n송금해요\\n\\n송금해요\\n\\n송금해요\\n\\n송금해요\\n\\n송금해요\\n\\n송금해요���'\n",
      "\n",
      "⏱️ 처리 시간: 3.86초\n",
      "\n",
      "[20] 문장: 누구한테 보낼까요?\n",
      "✅ 파싱된 결과:\n",
      "{\n",
      "  \"intent\": \"inquiry\",\n",
      "  \"response\": \"누가 보낼까요?\"\n",
      "}\n",
      "\n",
      "⏱️ 처리 시간: 3.87초\n"
     ]
    }
   ],
   "source": [
    "for i,ex in enumerate(sample):\n",
    "    input_text = ex[\"text\"]\n",
    "    print(f\"\\n[{i+1}] 문장: {input_text}\")\n",
    "\n",
    "    result = run_chatbot_inference(input_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c746fbf-2e90-4329-81f9-fd409c10f11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ intent 오답 (예상: 시스템응답, 예측: None)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'recipient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ intent 오답 (예상: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 예측: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_intent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 대상 비교\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_recipient \u001b[38;5;241m==\u001b[39m \u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     14\u001b[0m     correct_recipient \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✔ recipient 정답\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'recipient'"
     ]
    }
   ],
   "source": [
    "correct_intent = 0\n",
    "correct_recipient = 0\n",
    "correct_amount = 0\n",
    "    \n",
    "\n",
    "    pred_intent = result.get(\"intent\")\n",
    "    pred_recipient = result.get(\"recipient\")\n",
    "    pred_amount = int(result.get(\"amount\")) if result.get(\"amount\") is not None else None\n",
    "\n",
    "    # 의도 비교\n",
    "    if pred_intent == ex[\"intent\"]:\n",
    "        correct_intent += 1\n",
    "        print(\"✔ intent 정답\")\n",
    "    else:\n",
    "        print(f\"❌ intent 오답 (예상: {ex['intent']}, 예측: {pred_intent})\")\n",
    "\n",
    "    # 대상 비교\n",
    "    if pred_recipient == ex[\"slots\"][\"recipient\"]:\n",
    "        correct_recipient += 1\n",
    "        print(\"✔ recipient 정답\")\n",
    "    else:\n",
    "        print(f\"❌ recipient 오답 (예상: {ex['slots']['recipient']}, 예측: {pred_recipient})\")\n",
    "\n",
    "    # 금액 비교\n",
    "    if pred_amount == ex[\"slots\"][\"amount\"]:\n",
    "        correct_amount += 1\n",
    "        print(\"✔ amount 정답\")\n",
    "    else:\n",
    "        print(f\"❌ amount 오답 (예상: {ex['slots']['amount']}, 예측: {pred_amount})\")\n",
    "\n",
    "# 평가 결과 출력\n",
    "total = len(sample)\n",
    "print(\"\\n📊 평가 결과\")\n",
    "print(f\"Intent 정확도: {correct_intent}/{total} ({correct_intent/total:.0%})\")\n",
    "print(f\"Recipient 정확도: {correct_recipient}/{total} ({correct_recipient/total:.0%})\")\n",
    "print(f\"Amount 정확도: {correct_amount}/{total} ({correct_amount/total:.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db84a4-03b5-47ac-b430-f44c37d4a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2cf00-8bb3-49ce-84b3-50636ac9b433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
